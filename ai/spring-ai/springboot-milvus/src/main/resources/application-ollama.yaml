spring:
  # ollama相关配置
  ai:
    ollama:
      base-url: http://localhost:11434
      # 聊天配置
      chat:
        #        enabled: false
        options:
#          model: llama3.1:latest
          model: deepseek-r1:7b
          temperature: 0.7
      #          num-g-p-u: 1
      # 向量模型配置
      embedding:
        options:
          model: bge-m3:latest